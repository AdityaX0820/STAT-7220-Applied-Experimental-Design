---
title: "Experimental Design with Random Effects"
author: "Dr Austin R Brown"
institute: "Kennesaw State University"
format: beamer
editor: visual
execute:
  echo: true
  include: true
  warning: false
  message: false
  tidy: false
---

## Introduction

- Throughout this course, we have discussed various experimental designs and methods for analyzing data from these designs.

\vskip 0.15 in

- In general, they all have similar characteristics: we have a response variable that is measured quantitatively and we have one or more categorical factors that we are interested in comparing and/or controlling for.

\vskip 0.15 in

- In these designs, we've made a pretty big and unspoken assumption: we are interested in comparing the means of the response variable across the **specific** levels of the categorical factors.

## Introduction

- In other words, our categorical factors have been considered **fixed**. 

\vskip 0.15 in

- In this module, we will discuss a different type of categorical factor: **random** factors.

\vskip 0.15 in

- What in the world is a random factor? Let's dive in!

## Random Factors: A Definition

- A random factor is a categorical factor whose specific value is not necessarily of interest to the experimenter. 

\vskip 0.15 in

- For example, suppose we wanted to compare mean math scores of students in different schools.

\vskip 0.15 in

- We might be interested in the mean math scores of students in **all** schools, but we don't care about the specific schools we sampled from.
    - We just want to know if there is a difference in mean math scores between schools and these schools were (in theory) chosen at random from the larger population of all schools to be included in the experiment.
    
## Random Factors: A Definition

- To account for potential sources of variation in the schools, we might want to include the school as a random factor in our analysis.

\vskip 0.15 in

- Let's say we sampled 5 schools and within each school, we sampled 10 8th grade students. We collected their math scores on a state standardized test. The results are contained in `math_scores.xlsx`.

## One-Factor Random Factor Model

- Before we dig into the data, we need to consider how the ultimate analytical tools we will use differ from what we have used in the past.

\vskip 0.15 in

- In this particular case, we have one categorical and random factor (school) and one continuous response variable (math score).

\vskip 0.15 in

- As you might have guessed, the descriptive analyses don't really change from what we did in a one-factor CRD.
    - What about the inferential analyses?
    
## One-Factor Random Factor Model

- In a one-factor CRD, we used one-way ANOVA to test for differences in the means of the response variable across the levels of the categorical factor.

\vskip 0.15 in

- We use a very similar model here, but one that accounts for two sources of randomness: from the random factor and from the error term:

$$ y_{ij} = \mu + a_i + \epsilon_{ij} $$

where:

- $y_{ij}$ = the $j^{th}$ observation from the $i^{th}$ level of the random factor, $\mu$ = the overall mean, $a_i$ = the random effect of the $i^{th}$ level of the random factor, and $\epsilon_{ij}$ = the error term.

## One-Factor Random Factor Model

- Note that:

$$ a_i \sim N(0, \sigma_a^2) $$
$$ \epsilon_{ij} \sim N(0, \sigma^2) $$

## One-Factor Random Factor Model

- Further, remember from before and from regression that $MSE$ is an unbiased estimator of $\sigma^2$, i.e.,

$$ E[MSE] = \sigma^2 $$

- $MSA$, which is the mean square for the random factor, has the expected value of:

$$ E[MSA] = \sigma^2 + r \sigma_a^2 $$

## One-Factor Random Factor Model

- Just like with the fixed-effects one-way ANOVA model, our F-statistic was the ratio of $MSA$ to $MSE$. Here, if the teachers have no effect on the students' math scores (e.g., $\sigma^2_a = 0$), we would expect that:

$$ F_{\text{Stat}} = \frac{MSA}{MSE} = \frac{\sigma^2 + r \sigma_a^2}{\sigma^2} = 1 $$

- If the teachers do have an effect on the students' math scores (e.g., $\sigma^2_a > 0$), we would expect that:

$$ F_{\text{Stat}} = \frac{MSA}{MSE} = \frac{\sigma^2 + r \sigma_a^2}{\sigma^2} > 1 $$

## One-Factor Random Factor Model

- This leads us to our statistical hypotheses:

$$ H_0: \sigma^2_a = 0 $$
$$ H_1: \sigma^2_a > 0 $$

- We can use the F-statistic to test these hypotheses, in a similar manner to what we did with the one-way ANOVA model.

## One-Factor Random Factor Model

- This is well and good! But let's take a step back and think about how we would go about estimating $\sigma^2_a$.
    - This will help us as we work through the analysis.
    
\vskip 0.15 in

- Assuming we have $I$ levels of the random factor, we can estimate $\sigma^2_a$ using the following formula:

$$ \hat{\sigma}^2_a = \frac{MSA - MSE}{r} $$
$$ MSA = \frac{\sum_{i=1}^I r_i(\bar{y}_{i\cdot} - \bar{y}_{\cdot\cdot})^2}{I-1} $$
$$ MSE = \frac{\sum_{i=1}^I(r_i-1)s_i^2}{N-I} $$

## One-Factor Random Factor Model

- If $MSA = MSE$, then this suggests that the between group variation (i.e., the differences between group means and the grand mean) is equal to the within group variation (i.e., the differences between group means and the mean of the observations within each group).

\vskip 0.15 in

- If we go back to the fundamental ANOVA identity, ($SST = SSA + SSE$), the above case would generally suggest that $SSA \approx SSE$, which would mean that explained and unexplained variation are accounting for similar amounts of total variation.

\vskip 0.15 in

- Conversely, if $MSA > MSE$, then this means that $SSA > SSE$, which would suggest that the explained variation attributed to $SSA$ is greater than the unexplained variation attributed to $SSE$.

## One-Factor Random Factor Model

- Okay, now that we're through this theory, let's take a look at how we would perform data analysis structurally like we have done before.

\scriptsize
```{r}
library(tidyverse)
library(readxl)
library(rstatix)
## Read in the Data ##
math <- read_excel("math_scores.xlsx")
## Calculate Means and SDs by School ##
math |>
  group_by(School) |>
  get_summary_stats(MathScore,type='mean_sd') |>
  select(-variable)
```
\normalsize

## One-Factor Random Factor Model

- As we can see here, the mean math scores of students in School A and C are meaningfully greater than those in B, D, and E from a contextual perspective (e.g., moving from a C-D grade level to a B-C grade level).

\vskip 0.15 in

- We can visualize these data using a boxplot as we've done before.

## One-Factor Random Factor Model

```{r,eval=F}
math |>
  ggplot(aes(x=School,y=MathScore)) +
  geom_boxplot() +
  labs(title="Math Scores by School",
       x="School",
       y="Math Score") +
  theme_bw()
```

## One-Factor Random Factor Model

```{r,echo=F}
math |>
  ggplot(aes(x=School,y=MathScore)) +
  geom_boxplot() +
  labs(title="Math Scores by School",
       x="School",
       y="Math Score") +
  theme_bw()
```

## One-Factor Random Factor Model

- Again, we see that School C has the greatest mean scores and School B has the least mean scores, but all of the other groups exhibit some degree of overlap. 

\vskip 0.15 in

- Now let's move into our model building. Here, since we are working with random effects, we need to make use of a new R package called `lme4`.

```{r,eval=F}
install.packages('lme4')
```

## One-Factor Random Factor Model

- One thing you'll notice about the function in the `lmer` function below is that it looks a little different than our `aov` or `lm` function specifications. 

\vskip 0.15 in

- Here, we have a grand mean that we assume is fixed, $\mu$. Then we have $a_i$ values that we assume are random. Since we have no quantitative predictors, we can just use the intercept (1) as our fixed effect.
    - We call this a **random intercept** model since $\mu + a_i$ is the intercept of the model for each group.
    
```{r}
library(lme4)
## Fit the model ##
random_effects_model <- lmer(MathScore ~ 1 + (1|School), data = math)
```

## One-Factor Random Factor Model

- We can still check the assumptions of normality and constant variance in the exact same manners as we've done all semester (e.g, QQ plots, residuals vs. fitted values, etc.).
    - I'll skip that part in the slides but know that you can do this in the same way as before.
    
\vskip 0.15 in

- So how do we interpret the output with respect to our statistical hypotheses?
    - The output of the `lmer` function is a little different than what we have seen before.
    
\vskip 0.15 in

- Here, we will use the `lmerTest` package and the `ranova` function within the package.
    
## One-Factor Random Factor Model

```{r}
#install.packages('lmerTest')
library(lmerTest)
ranova(random_effects_model)
```

## One-Factor Random Factor Model

- Here, this is returning to us a table that is similar to the ANOVA table we have seen before.
    - The exception is that we are running a full vs reduced type of test (sometimes called "hierarchical testing") to test the significance of the random factor.
    
\vskip 0.15 in

- The significant p-value suggests that the random factor (school) does have a significant effect on the response variable (math score).

\vskip 0.15 in

- We can also extract the variance components from the model using the `VarCorr` function.
    
## One-Factor Random Factor Model

```{r}
var_components <- VarCorr(random_effects_model)
var_components
```
## One-Factor Random Factor Model

- You may be asking yourself, okay so now the next step should be running a post-hoc test, right?

\vskip 0.15 in

- Not quite! For fixed-effect models, yes we would do this, but for random effects models, we are not interested in comparing the means of the groups since we don't necessarily care about the specific groups we sampled from.

\vskip 0.15 in

- Instead, we are interested in estimating the variance components of the random factor and the error term.
    - This is similar to what we did with the one-way ANOVA model, but here we are estimating the variance components of the random factor and the error term.
    
## One-Factor Random Effects Model

- So for us, the variance of the random intercepts for schools is $\sigma^2_a = 9.5416^2 = 91.04$.
    - For the error term, we have $\sigma^2 = 9.9391^2 = 98.79$. 
    
\vskip 0.15 in

- In terms of the schools, this is telling us what we saw visually: scores between schools seem to vary, on average, about 9.54 points from the grand mean.
    - This is a pretty large amount of variation (a full letter grade!), so we might want to consider this in our future analyses.
    
## Conclusions

- In conclusion, we have seen the similarities and differences in a random factor and fixed factor model.

\vskip 0.15 in

- In practice, instances where you would want to employ a random effects model might include different raters or measurement devices.

\vskip 0.15 in

- For example, different judges in a competition, different measurement devices, or different schools in a school district.
    - In these cases, we are not necessarily interested in the specific judges, devices, or schools, but rather the overall effect of the random factor on the response variable.
    


{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62032799",
   "metadata": {},
   "source": [
    "# Intro to Python Programming for Data Science\n",
    "## Dr Austin R Brown\n",
    "## School of Data Science and Analytics\n",
    "### Kennesaw State University"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba95023e",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/abrown9008/STAT-7220-Applied-Experimental-Design/blob/main/Python%20for%20Data%20Science/Introduction%20to%20Python%20for%20Data%20Science.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace21525",
   "metadata": {},
   "source": [
    "## Why Learn Python?  \n",
    "- You may be asking yourself, out of all of the possible programming languages which exist, why should I spend time learning Python?\n",
    "\n",
    "- Great question!\n",
    "\n",
    "- Python is a useful tool and worthwhile to learn for several reasons:\n",
    "    1. It's free!\n",
    "    2. Because it's open source, thousands of people have contributed packages and functions at a pace that proprietary softwares can't compete with\n",
    "    3. It is a very flexible and robust general programming language, meaning there's a lot you can do with it in the data science space and beyond!\n",
    "    4. It has become basically the standard in industry\n",
    "\n",
    "## So What is Python?\n",
    "\n",
    "- Python is command-line, object-oriented general programming language commonly used for data analysis, data science and statistics.\n",
    "\n",
    "- **Command-line** means that we have to give it commands in order for us to get it to do something. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4a011d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## What is the sum of 2 & 3? ##\n",
    "2 + 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e1b5db",
   "metadata": {},
   "source": [
    "**Object-oriented** means that we can save individual pieces of output as some name that we can use later. This is a super handy feature, especially when you have complicated scripts! For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1f97bae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "## Save 2 + 3 as \"a\" ##\n",
    "a = 2 + 3\n",
    "print(a)\n",
    "print(a*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c203d1",
   "metadata": {},
   "source": [
    "## What Can Python do?\n",
    "\n",
    "- What can Python do? Well, for the purpose of data analytics, I have yet to find a limit!\n",
    "\n",
    "- In this class, we will be learning how to use Python as a tool in the data science workflow with specific attention placed on designing and evaluating experiments (more on that in the first week's lecture!)\n",
    "\n",
    "- What is the data science workflow? Let's take a look!\n",
    "\n",
    "![From R for Data Science 2nd Edition](Data%20Science%20Workflow.png)\n",
    "\n",
    "## Importing Data/Data Loading\n",
    "\n",
    "- Since a major reason we use Python is for the analysis of data, we need to know how to import/load data from various sources and file formats into our Python programming environment.\n",
    "\n",
    "- There are a variety of ways of importing data into our Python programming environment, which largely depend on the type of datafile that you are importing (e.g., Excel file, CSV file, text file, SAS dataset, SPSS dataset, etc.).\n",
    "\n",
    "- While there are lots of different files which can be imported into our Python programming environment (Google/GenAI is an excellent resource for searching for code for how to start to do something), we're going to focus on two main types: Excel and CSV\n",
    "\n",
    "- For example, let's try importing a CSV file using Python. This file is part of the famous Framingham Heart Study and is called `HEART.csv` and is located in the `Python for Data Science` subfolder that's part of our class GitHub repo.\n",
    "\n",
    "- To read in this CSV file using Python, we will use the `read_csv` function, which is part of the famous `pandas` package.\n",
    "\n",
    "### Defining Packages and Functions\n",
    "\n",
    "- Okay, but before we get into reading in the `HEART` CSV file, what in the world is a package and function??\n",
    "\n",
    "- We can think of packages like toolboxes in a mechanic's shop. Each toolbox contains different tools used for specific purposes.\n",
    "\n",
    "- To access a particular tool, we have to go to the right toolbox.\n",
    "    - A toolbox is like a package\n",
    "    - The tools within the toolbox are like functions within a package\n",
    "\n",
    "- Thus, `read_csv` is a tool (function) within the `pandas` toolbox (package).\n",
    "\n",
    "- A function can also be thought of like a mathematical function: we provide some input and some specific output is returned. Now, while our Python programming enviroment comes with some functions pre-loaded, almost all others in existence have to be installed from the web, including `pandas`.\n",
    "\n",
    "- To install a Python package, we have to use a particular command line function called `pip` which is a recursive acroymn for \"pip installs packages\". \n",
    "    - In brief, `pip` is a package management system used to install and manage software packages written in Python.\n",
    "\n",
    "- Since we need `pandas` to load the `HEART.csv` file, we can install by using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ea6486b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install pandas using pip ##\n",
    "## Remove the # to the left of %pip\n",
    "## before executing the code ##\n",
    "\n",
    "#%pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a81cb4",
   "metadata": {},
   "source": [
    "- Now, we can load the `pandas` library into our current Python environment by using the `import` function. Note, to access the functions within a package, we have to use the following code syntax: `package_name.function_name`\n",
    "\n",
    "- So typically when we import a package, we shorten its name to something to allow for brevity in our code.\n",
    "    - `pandas` is almost universally imported as `pd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "51e3e139",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load pandas library ##\n",
    "import pandas as pd\n",
    "\n",
    "## Load HEART CSV ##\n",
    "heart = pd.read_csv(\"HEART.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3386bc2d",
   "metadata": {},
   "source": [
    "- Awesome! Now that we've loaded the `heart` dataframe, how do we know that it loaded correctly?\n",
    "\n",
    "- There are two general approaches I'd recommend. One is a simple overview of the first few rows of the dataframe which we can obtain via the `.head` function.\n",
    "\n",
    "- The second is by using the `.info` function. This is equivalent to `dplyr::glimpse` in R or `PROC CONTENTS` in SAS.\n",
    "    - Let's check out how we'd use both techniques here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "16b8d5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5209, 17)\n"
     ]
    }
   ],
   "source": [
    "## First, what are the number of rows\n",
    "## and columns in the dataframe?\n",
    "## Let's use the .shape function! ##\n",
    "\n",
    "print(heart.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe8deb7",
   "metadata": {},
   "source": [
    "- Nice! So we know we have 5209 rows (or observations) and 17 columns (or variables).\n",
    "\n",
    "- Now let's check out the techniques for inspecting the dataframe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "70b90a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Status DeathCause  AgeCHDdiag     Sex  AgeAtStart  Height  Weight  \\\n",
      "0   Dead      Other         NaN  Female          29   62.50   140.0   \n",
      "1   Dead     Cancer         NaN  Female          41   59.75   194.0   \n",
      "2  Alive        NaN         NaN  Female          57   62.25   132.0   \n",
      "3  Alive        NaN         NaN  Female          39   65.75   158.0   \n",
      "4  Alive        NaN         NaN    Male          42   66.00   156.0   \n",
      "\n",
      "   Diastolic  Systolic    MRW  Smoking  AgeAtDeath  Cholesterol  \\\n",
      "0         78       124  121.0      0.0        55.0          NaN   \n",
      "1         92       144  183.0      0.0        57.0        181.0   \n",
      "2         90       170  114.0     10.0         NaN        250.0   \n",
      "3         80       128  123.0      0.0         NaN        242.0   \n",
      "4         76       110  116.0     20.0         NaN        281.0   \n",
      "\n",
      "  Chol_Status BP_Status Weight_Status   Smoking_Status  \n",
      "0         NaN    Normal    Overweight       Non-smoker  \n",
      "1   Desirable      High    Overweight       Non-smoker  \n",
      "2        High      High    Overweight  Moderate (6-15)  \n",
      "3        High    Normal    Overweight       Non-smoker  \n",
      "4        High   Optimal    Overweight    Heavy (16-25)  \n"
     ]
    }
   ],
   "source": [
    "## .head method ##\n",
    "\n",
    "print(heart.head(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414e401e",
   "metadata": {},
   "source": [
    "- With `.head`, we can see that the variable `Sex`, for example, is a *categorical* variable, meaning that it's values are qualities (e.g., `Male` or `Female`).\n",
    "\n",
    "- On the other hand, `Height`, seems to be measured with numbers likely implying that it is a *quantitative* variable.\n",
    "    - We can use `.info` and the `pyjanitor` package to confirm this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d32ece25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5209 entries, 0 to 5208\n",
      "Data columns (total 17 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Status          5209 non-null   object \n",
      " 1   DeathCause      1991 non-null   object \n",
      " 2   AgeCHDdiag      1449 non-null   float64\n",
      " 3   Sex             5209 non-null   object \n",
      " 4   AgeAtStart      5209 non-null   int64  \n",
      " 5   Height          5203 non-null   float64\n",
      " 6   Weight          5203 non-null   float64\n",
      " 7   Diastolic       5209 non-null   int64  \n",
      " 8   Systolic        5209 non-null   int64  \n",
      " 9   MRW             5203 non-null   float64\n",
      " 10  Smoking         5173 non-null   float64\n",
      " 11  AgeAtDeath      1991 non-null   float64\n",
      " 12  Cholesterol     5057 non-null   float64\n",
      " 13  Chol_Status     5057 non-null   object \n",
      " 14  BP_Status       5209 non-null   object \n",
      " 15  Weight_Status   5203 non-null   object \n",
      " 16  Smoking_Status  5173 non-null   object \n",
      "dtypes: float64(7), int64(3), object(7)\n",
      "memory usage: 691.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## .info method ##\n",
    "\n",
    "print(heart.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5389521b",
   "metadata": {},
   "source": [
    "- In this output, look again at the `Sex` variable. We see that it has 5209 non-null (non-missing) values, meaning that for every row in the dataframe, we have a valid, non-missing value!\n",
    "\n",
    "- Then next to this information, we can see that its datatype is `object`. This is Python's naming convention for a nominal, categorical variable.\n",
    "\n",
    "- For `Height`, we see that is has 6 missing values ($5209 - 5203 = 6$). We also see that Python considers it `float64`. This is Python's naming convention for a continuous, quantitative variable.\n",
    "    - Notice that `AgeAtStart` is considered `int64`. This is an integer, or discrete, quantitative variable designation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49072e9",
   "metadata": {},
   "source": [
    "## Working with Dataframes\n",
    "\n",
    "- Let's say I wanted to find the average or mean of the `AgeAtStart` column from the `heart` dataframe. How would I go about doing that?\n",
    "\n",
    "- First, I need to know how to refer to that single variable by itself.\n",
    "\n",
    "- To do this, we make use of the square bracket notation. Specifically, we can call a single column within a dataframe by using the following syntax: `df['Variable_Name']`.\n",
    "\n",
    "- You can think of the square bracket like a door to your home. The name of the dataframe is the house itself, the brackets are the door, and the variable name is the person we want to talk to inside of the house.\n",
    "    - So the structure is `house['Person']`.\n",
    "\n",
    "- If we run the following command, let's see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8eb118a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       29\n",
      "1       41\n",
      "2       57\n",
      "3       39\n",
      "4       42\n",
      "        ..\n",
      "5204    49\n",
      "5205    42\n",
      "5206    51\n",
      "5207    36\n",
      "5208    36\n",
      "Name: AgeAtStart, Length: 5209, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## house[\"Person\"] ##\n",
    "\n",
    "print(heart['AgeAtStart'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd58c672",
   "metadata": {},
   "source": [
    "- If we go back and look at the output of the `.head` function, we can see that the values in this output vector correspond to the output we had from that function as well.\n",
    "\n",
    "- Note, the first element in Python (and many other programming languages) is coded as 0 rather than 1. There are some historical reasons for this but it ends up serving convenience purposes in some operations (i.e., range and length calculations) as well.\n",
    "\n",
    "- Okay so now that we've established how to directly refer to a variable within a dataframe, how do we calculate the mean? Here, we are going to use the popular `numpy` package to help us out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f084455a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.07\n"
     ]
    }
   ],
   "source": [
    "## Install numpy. Remember\n",
    "## to remove # from the left\n",
    "## of the following line of \n",
    "## code before executing ##\n",
    "\n",
    "#%pip install numpy\n",
    "\n",
    "## Load numpy ##\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "## Calculate Mean AgeAtStart ##\n",
    "\n",
    "print(round(\n",
    "    np.mean(heart['AgeAtStart']),\n",
    "    2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95625ee",
   "metadata": {},
   "source": [
    "- So the mean age at start is 44.07 years old!\n",
    "\n",
    "## Tidying Data\n",
    "\n",
    "### Selecting Columns\n",
    "\n",
    "- Now, let's say I have a large dataframe with lots of columns of information, as you might see in your own careers.\n",
    "\n",
    "- But, for whatever analysis I'm wanting to do, I don't need all of the columns, just a few.\n",
    "\n",
    "- In such a case, it might be useful to subset the dataframe and select only the columns we need.\n",
    "\n",
    "- How do we go about doing this? Like many things in Python, there are a few different ways to yield the same result, but I'm going to show you what I consider the most straightforward method, which uses the `house[\"Person\"]` syntax we worked with previously.\n",
    "\n",
    "- Let's say using the `heart` dataframe, I want to create a new dataframe which only contains the last four columns: `Chol_Status`, `BP_Status`, `Weight_Status`, and `Smoking_Status`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f1f80e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5209 entries, 0 to 5208\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   Chol_Status     5057 non-null   object\n",
      " 1   BP_Status       5209 non-null   object\n",
      " 2   Weight_Status   5203 non-null   object\n",
      " 3   Smoking_Status  5173 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 162.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## Creat New Dataframe using subset of \n",
    "## columns ##\n",
    "\n",
    "heart1 = heart[['Chol_Status',\n",
    "                'BP_Status',\n",
    "                'Weight_Status',\n",
    "                'Smoking_Status']]\n",
    "\n",
    "print(heart1.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a348490",
   "metadata": {},
   "source": [
    "- We can see that the column selection worked because we have the same number of rows (5209) but now only those four columns we specified.\n",
    "\n",
    "### Filtering Rows\n",
    "\n",
    "- We just learned how to subset columns. What if we wanted to subset by values in the rows?\n",
    "\n",
    "- For example, let's say in the new `heart1` dataframe we just created, we want to create a new dataframe where we only have those participants whose `Weight_Status` is \"Overweight.\"\n",
    "\n",
    "- Again, there are a few different approaches, but I would recommend using what we've learned so far with the `house['Person']` syntax.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "421a2b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3550, 4)\n"
     ]
    }
   ],
   "source": [
    "## Create new dataframe from heart1\n",
    "## where participants' Weight_Status == \"Overweight\" ##\n",
    "\n",
    "heart2 = heart1[heart1['Weight_Status'] == \"Overweight\"]\n",
    "\n",
    "print(heart2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a9b45a",
   "metadata": {},
   "source": [
    "- Here we can see that the number of columns is the same (4) but now the number of rows is smaller than the total number in the `heart1` dataframe.\n",
    "    - How can we confirm that the filtering worked correctly?\n",
    "\n",
    "- One strategy is by counting up the number of participants in the `heart1` dataframe whose `Weight_Status` was considered \"Overweight\". \n",
    "\n",
    "- If that number matches the number of rows in `heart2`, then we can feel confident that the filtering worked the way we expected it to.\n",
    "    - Let's try using the `.value_counts` function to check our work!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "97afb0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight_Status\n",
      "Overweight     3550\n",
      "Normal         1472\n",
      "Underweight     181\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Tabulate Number of Overweight Participants\n",
    "## in heart1 dataframe ##\n",
    "\n",
    "print(heart1['Weight_Status'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742d3057",
   "metadata": {},
   "source": [
    "- Since the number of Overweight participants in `heart1` (3550) is equal to the number of rows in `heart2`, we can feel confident our filtering worked the way we anticipated!\n",
    "\n",
    "## What is \"Tidy\" Data?\n",
    "\n",
    "- Once we have imported data, our next job is often to \"tidy\" it. Tidy data refers to data structure or how information is stored.\n",
    "\n",
    "- A tidy dataframe has the following characteristics:\n",
    "    1. Each variable is a column; each column is a variable.\n",
    "    2. Each observation is a row; each row is an observation.\n",
    "    3. Each value has is a cell; each cell is a single value.\n",
    "\n",
    "![From R for Data Science 2nd Edition](Tidy%20Data%20Structure.png)\n",
    "\n",
    "- Why should we care about having our data in tidy format? There are two key reasons:\n",
    "\n",
    "- First, consistency. It's much easier to work with datasets if we know what format they're in.\n",
    "\n",
    "- Second, this is generally the structure most Python functions want the data to be in to work correctly."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
